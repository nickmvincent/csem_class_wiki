<!-- TITLE: Algorithmic Bias -->
<!-- SUBTITLE: A quick summary of Bias -->

# Introduction:
## What is an algorithm? 

* When you hear the word algorithm, what comes to mind? What are some examples of algorithms?
	* Math problems
	* Recipes 
	* Computer Programs
* Formalized Definition of an Algorithm from Merriam Webster Dictionary
	* A procedure for solving a mathematical problem in a finite number of steps that frequently involves repetition of an operation
	* Broadly : a step-by-step procedure for solving a problem or accomplishing some end
* Informal definition
	* it’s basically logic that is formalized and repeatable, so the same inputs create the same outputs. 

# Activity 1: Create Algorithm for getting dinner
## What logic goes into creating an  algorithm?
* Formulate an informal algorithm for how we would eat dinner together - do this with the group. Below is just an example: 
	* For example: What am I going to eat for lunch? 
	* How hungry am I? 
		* If large hunger, 
			* If I have chicken
				* Chicken 
			* If I have salmon
				* Salmon
			* If I’m near Tomate
				* Burrito
		* If average hunger,
			* If I want to save some for later
				* Sandwich 
			* If I’m near Chipotle
				* Rice bowl
			* If I don’t have much time
				* Stir fry veggies
			* If I’m having a bad day
				* Pasta 
		* If little hunger, 
			* If I want to eat healthy
				* Salad 
			* If I want to stay light
				* Smoothie 
			* If I woke up late, 
				* Bagel 
* Note: That’s similar to how algorithms work, but with a lot more history to base it off of. So, in our previous example, maybe we found out that the last time we ate a bagel, we got way too full. This time, we won’t eat a bagel when we’re only a little hungry; we’ll eat a bagel when we’re average-level hungry. 

# Activity 2: Discuss what this process might entail when Creating a More Serious Algorithm
Now, let’s think about how we’d create an algorithm to help prevent crime by trying to predict which individuals are more likely to commit crime and subsequently place more police in that location. That’s definitely seems like a noble goal right?
But what questions are they asking to decide which individuals are more likely to commit crime?
What else is problematic or concerning about this type of algorithm?
Answer: When it’s applied to systems that can perpetuate prejudice, often it does and often it creates what we refer to as “Algorithmic Bias”.
There’s a system called “LSI–R, or Level of Service Inventory–Revised,” which helps judges make decisions on how dangerous a convict might be. It has many of the same questions that we discussed earlier. 
The Level of Service Inventory – Revised (LSI-R) is an actuarial assessment tool designed to identify the offenders' risks and needs with regard to recidivism. That is, the LSI-R seeks to classify an offender's risk of re-offending as well as to identify their particular criminogenic needs.
However, these questions that point more closely to the neighborhood a person is from, the records of peers living in their neighborhood and the current amount of police presence in the neighborhood
Those of lower socioeconomic background are currently labeled  with higher crime rates and with this algorithm, these neighborhood will continue to be labeled as dangerous
 No matter how sound the algorithm seems, or the “confidence level” it spits out, it produces a self perpetuating cycle, albeit with good intentions. 

# Discussion: Reviewing Algorithms and Algorithmic Bias that affect our everyday lives
In the sites that we use everyday browsing the internet
* Google 
* Facebook

Two examples
## Activity: Google Search images of CEO 
Google did it
Research Study: To come to those conclusions, Datta and his colleagues basically built a tool, called Ad Fisher, that tracks how user behavior on Google influences the personalized Google ads that each user sees. Because that relationship is complicated and based on a lot of factors, the researchers used a series of fake accounts: theoretical job-seekers whose behavior they could track closely. That online behavior — visiting job sites and nothing else — was the same for all the fake accounts. But some listed their sex as men and some as women.
“The Ad Fisher team found that when Google presumed users to be male job seekers, they were much more likely to be shown ads for high-paying executive jobs. Google showed the ads 1,852 times to the male group — but just 318 times to the female group.”
“It’s part of a cycle: How people perceive things affects the search results, which affect how people perceive things,” Cynthia Matuszek, a computer ethics professor at University of Maryland
Other Google Examples:
This isn’t the first time that algorithm systems have appeared to be sexist — or racist, for that matter. When Flickr debuted image recognition tools in May, users noticed the tool sometimes tagged black people as “apes” or “animals.” 
A landmark study at Harvard previously found serious discrimination in online ad delivery, like when searching ethnic names on Google turned up more results around arrest records. 
Activity: Google Search three black men, three asian men and compare google image results
Discussion: Who is Responsible for these results
“How much of this is us and how much of this is baked into the algorithm? It’s a question that a lot of people are struggling to answer?”
After all, algorithmic personalization systems, like the ones behind Google’s ad platform, don’t operate in a vacuum: They’re programmed by humans and taught to learn from user behavior. So the more we click or search or generally Internet in sexist, racist ways, the algorithms learn to generate those results and ads (supposedly the results we would expect to see).

## Facebook did it too:
They can hide behind a terms and conditions that says their ad posters should all be compliant, but at the end of the day, their algorithms facilitate this too.
Discussion: How much of this is on Facebook and how much of this is on the company requesting the advertisements to target a certain population?
Activity: Look at ad categories on Facebook. Go to settings > ads > your information > your categories. 
Discussion: What does this say about you? What kinds of ads could you be receiving or not receiving because of this? 
China’s social credit
By 2020, everyone in China will be enrolled in a vast national database that compiles fiscal and government information, including minor traffic violations, and distils it into a single number ranking each citizen.
Users are encouraged to flaunt their good credit scores to friends, and even potential mates. China's biggest matchmaking service, Baihe, has teamed up with Sesame to promote clients with good credit scores, giving them prominent spots on the company's website.
"Someone who plays video games for 10 hours a day, for example, would be considered an idle person, and someone who frequently buys diapers would be considered as probably a parent, who on balance is more likely to have a sense of responsibility," Li Yingyun, Sesame's technology director told Caixin, a Chinese magazine, in February.
A national database will merge a wide variety of information on every citizen, assessing whether taxes and traffic tickets have been paid, whether academic degrees have been rightly earned and even, it seems, whether females have been instructed to take birth control.	
"Without a system, a conman can commit a crime in one place and then do the same thing again in another place. But a credit system puts people's past history on the record. It'll build a better and fairer society," she promises.
Discussion:
Based on your knowledge of the U.S., how would this algorithm affect people from a lower socioeconomic neighborhood? Infertile couples? Video game developers? 
While extreme, this example is essentially what’s happening in ad networks and search results, which changes the information we see and the things that are marketed to us (and thus, our consumption habits).

# Final Discussion: Solution to Algorithmic Bias?
## After today’s discussion, how would you define Algorithmic Bias?
* How do you think we can work towards eliminating algorithmic bias?
	* Band-aid vs. actual solution
		* What are the responsibilities of the major corporations (Google, Facebook)?
		* What are the responsibilities of the companies requesting ads?
		* What is our responsibility as a consumer?
	* How much can we actually ask companies to change, given that they benefit from this infrastructure?
* What can we do to help?
	* Ask for evidence for the algorithms we take part in
	* MuckRock
		* We have the power and the right to demand information in many situations, and it’s up to us to decide how or where or when.
			*  Explain what MuckRock is, and the resources it provides.
			*  Explain why working with MuckRock might be hard, but it’s worth it
			* Talk about the impacts of MuckRock
	* How to file a MuckRock claim



